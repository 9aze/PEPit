import cvxpy as cp
import numpy as np

from PEPit.pep import PEP
from PEPit.Function_classes.convex_function import ConvexFunction
from PEPit.Function_classes.convex_indicator import ConvexIndicatorFunction
from PEPit.Primitive_steps.bregmangradient_step import BregmanGradient_Step


def wc_no_lips2(L, gamma, n):
    """
    In this example, we use a Bregman gradient method for
    solving the constrained smooth strongly convex minimization problem
        min_x { F(x) = f_1(x) + f_2(x) }
    for notational convenience we denote xs=argmin_x F(x);
    where f_1(x) is h-smooth and convex and where f_2(x) is a closed convex
    indicator function.

    We show how to compute the worst-case value of F(xN)-F(xs) when xN is
    obtained by doing N steps of the method starting with an initial
    iterate satisfying Dh(x*,x0)<=1. (Dh is the Bregman distance generated by
    h, between x* and x0)

    [1] Heinz H. Bauschke, Jérôme Bolte, and Marc Teboulle. "A Descent Lemma
         Beyond Lipschitz Gradient Continuity: First-Order Methods Revisited
         and Applications." (2017)

    [2] Radu-Alexandru Dragomir, Adrien B. Taylor, Alexandre d’Aspremont, and
         Jérôme Bolte. "Optimal Complexity and Certification of Bregman
         First-Order Methods". (2019)

    DISCLAIMER: This example requires some experience with PESTO and PEPs
    (see Section 4 in [2]).

    :param L: (float) relative-smoothness parameter
    :param gamma: (float) step size.
    :param n: (int) number of iterations.
    :return:
    """

    # Instantiate PEP
    problem = PEP()

    # Declare a convex lipschitz function
    d = problem.declare_function(ConvexFunction,
                                    {})
    func1 = problem.declare_function(ConvexFunction,
                                     {})
    h = (d + func1)/L
    func2 = problem.declare_function(ConvexIndicatorFunction,
                                     {'R' : np.inf, 'D' : np.inf})
    func = func1 + func2

    # Start by defining its unique optimal point and its function value
    xs = func.optimal_point()
    Fs = func.value(xs)
    gfs, fs = func1.oracle(xs)
    ghs, hs = h.oracle(xs)

    # Then Define the starting point of the algorithm
    x0 = problem.set_initial_point()
    gh0, h0 = h.oracle(x0)
    gf0, f0 = func1.oracle(x0)

    # Set the initial constraint that is the distance between x0 and x^*
    problem.set_initial_condition(hs - h0 - gh0 * (xs - x0) <= 1)

    # Compute trajectory starting from x0
    x1, x2 = x0, x0
    gfx = gf0
    ghx = gh0
    hx1, hx2 = h0, h0
    for i in range(n):
        x1 = x2
        x2, _, _ = BregmanGradient_Step(gfx, ghx, func2 + h, gamma)
        gfx, _ = func1.oracle(x2)
        ghx, _ = h.oracle(x2)
        Dhx = hx1 - hx2 - ghx*(x2 - x1)
        # Set the performance metric to the final distance to optimum
        problem.set_performance_metric(Dhx)

    # Solve the PEP
    wc = problem.solve(solver=cp.MOSEK)

    # Theoretical guarantee (for comparison)
    theory = 2/(n-1)/n
    print('*** Example file: worst-case performance of the NoLips_2 in Bregman distance ***')
    print('\tPEP-it guarantee:\t Dh <= ', wc)
    print('\tTheoretical guarantee on quadratics = 1 :\tDh <=  <= ', theory)
    # Return the worst-case guarantee of the evaluated method (and the upper theoretical value)

    # Return the rate of the evaluated method
    return wc


if __name__ == "__main__":

    L = 1
    gamma = 1/L
    n = 10

    rate = wc_no_lips2(L=L,
                        gamma=gamma,
                        n=n)